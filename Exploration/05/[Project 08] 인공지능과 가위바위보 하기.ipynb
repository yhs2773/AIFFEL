{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c03026c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from PIL import Image\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad8444e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700  images to be resized.\n",
      "700  images resized.\n",
      "700  images to be resized.\n",
      "700  images resized.\n",
      "700  images to be resized.\n",
      "700  images resized.\n"
     ]
    }
   ],
   "source": [
    "# define function to easily resize given images\n",
    "def resize_images(img_path):\n",
    "    images=glob.glob(img_path + \"/*.jpg\") # file name to list, '*' for full text\n",
    "    \n",
    "    print(len(images), \" images to be resized.\")\n",
    "\n",
    "    # reshaping all images to 28x28 and then saving each one of them\n",
    "    target_size=(28, 28)\n",
    "    for img in images:\n",
    "        old_img=Image.open(img) # opening image\n",
    "        new_img=old_img.resize(target_size,Image.ANTIALIAS) # resizing\n",
    "        new_img.save(img, \"JPEG\") # saving image\n",
    "    \n",
    "    print(len(images), \" images resized.\")\n",
    "\n",
    "# loading and resizing all images in scissor folder\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/scissors\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "# loading and resizing all images in rock folder\n",
    "image_dir_path = os.getenv('HOME') + '/aiffel/rock_scissor_paper/rock'\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "# loading and resizing all images in rock folder\n",
    "image_dir_path = os.getenv('HOME') + \"/aiffel/rock_scissor_paper/paper\"\n",
    "resize_images(image_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f18d56fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 2100 입니다.\n",
      "x_train shape: (2100, 28, 28, 3)\n",
      "y_train shape: (2100,)\n"
     ]
    }
   ],
   "source": [
    "# function to load all train data\n",
    "def load_data(img_path, number_of_data=2100):  # ensure to double check the num of data\n",
    "    img_size=28\n",
    "    color=3\n",
    "    # assigning imgs with zero only matrix of total params\n",
    "    # scissors : 0, rock : 1, paper : 2\n",
    "    \n",
    "    # 1-d zero matrix of all params, then reshaping to 4-d tensor\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    # zero matrix for target\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissors/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32) # image data into np.array\n",
    "        imgs[idx,:,:,:]=img    # allocate img (array) to imgs index\n",
    "        labels[idx]=0   # scissors : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # allocate img (array) to imgs index\n",
    "        labels[idx]=1   # rock : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # allocate img (array) to imgs index\n",
    "        labels[idx]=2   # paper : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "(x_train, y_train) = load_data(image_dir_path)\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18513c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# quick check of the image\n",
    "plt.imshow(x_train[0])\n",
    "print('라벨: ', y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ead1c28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최소값: 0  최대값: 255\n",
      "최소값: 0  최대값: 255\n",
      "(1785, 28, 28, 3) (315, 28, 28, 3) (1785,) (315,)\n"
     ]
    }
   ],
   "source": [
    "# all image and label loaded into x_train and y_train\n",
    "# thus splitting parts of them as test sets using sklearn train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.15, stratify=y_train)\n",
    "\n",
    "print('최소값:',np.min(x_train), ' 최대값:',np.max(x_train)) # check min max of train data\n",
    "print('최소값:',np.min(x_test), ' 최대값:',np.max(x_test)) # check min max of test data\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aefe3a69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최소값: 0.0  최대값: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1785, 28, 28, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalizing train set\n",
    "x_train_norm = x_train / np.max(x_train) # divide by max num to normalize data\n",
    "print('최소값:',np.min(x_train_norm), ' 최대값:',np.max(x_train_norm)) # after normalizing\n",
    "x_train_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "256f998c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최소값: 0.0  최대값: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(315, 28, 28, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalizing test set\n",
    "x_test_norm = x_test / np.max(x_test)   # divide by max num to normalize data\n",
    "print('최소값:',np.min(x_test_norm), ' 최대값:',np.max(x_test_norm)) # after normalizing\n",
    "x_test_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89c1b462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model에 추가된 Layer 개수:  7\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 122,051\n",
      "Trainable params: 122,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# constructing a model using keras\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "print('Model에 추가된 Layer 개수: ', len(model.layers))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e6658c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "48/48 [==============================] - 3s 9ms/step - loss: 1.0941 - accuracy: 0.3691 - val_loss: 1.0859 - val_accuracy: 0.4552\n",
      "Epoch 2/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 1.0686 - accuracy: 0.4318 - val_loss: 1.0935 - val_accuracy: 0.3209\n",
      "Epoch 3/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 1.0203 - accuracy: 0.4779 - val_loss: 0.9606 - val_accuracy: 0.5597\n",
      "Epoch 4/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.9323 - accuracy: 0.5544 - val_loss: 0.8690 - val_accuracy: 0.6231\n",
      "Epoch 5/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.8708 - accuracy: 0.5900 - val_loss: 0.8329 - val_accuracy: 0.5896\n",
      "Epoch 6/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.8215 - accuracy: 0.6157 - val_loss: 0.7970 - val_accuracy: 0.6642\n",
      "Epoch 7/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.8098 - accuracy: 0.6355 - val_loss: 0.7542 - val_accuracy: 0.6791\n",
      "Epoch 8/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.7662 - accuracy: 0.6539 - val_loss: 0.7455 - val_accuracy: 0.6791\n",
      "Epoch 9/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.7632 - accuracy: 0.6638 - val_loss: 0.7325 - val_accuracy: 0.6866\n",
      "Epoch 10/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.7312 - accuracy: 0.6803 - val_loss: 0.7230 - val_accuracy: 0.6940\n",
      "Epoch 11/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.7117 - accuracy: 0.7007 - val_loss: 0.7371 - val_accuracy: 0.6791\n",
      "Epoch 12/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.7027 - val_loss: 0.6826 - val_accuracy: 0.6866\n",
      "Epoch 13/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.6560 - accuracy: 0.7304 - val_loss: 0.6606 - val_accuracy: 0.7164\n",
      "Epoch 14/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.6399 - accuracy: 0.7436 - val_loss: 0.6465 - val_accuracy: 0.7276\n",
      "Epoch 15/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.6430 - accuracy: 0.7456 - val_loss: 0.7367 - val_accuracy: 0.6903\n",
      "Epoch 16/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.6045 - accuracy: 0.7548 - val_loss: 0.6856 - val_accuracy: 0.7425\n",
      "Epoch 17/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.5876 - accuracy: 0.7581 - val_loss: 0.6458 - val_accuracy: 0.7164\n",
      "Epoch 18/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.5664 - accuracy: 0.7831 - val_loss: 0.6056 - val_accuracy: 0.7425\n",
      "Epoch 19/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.5342 - accuracy: 0.7963 - val_loss: 0.6177 - val_accuracy: 0.7649\n",
      "Epoch 20/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.5377 - accuracy: 0.7904 - val_loss: 0.6356 - val_accuracy: 0.7649\n",
      "Epoch 21/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.5131 - accuracy: 0.8082 - val_loss: 0.5662 - val_accuracy: 0.7948\n",
      "Epoch 22/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.4608 - accuracy: 0.8247 - val_loss: 0.5989 - val_accuracy: 0.7575\n",
      "Epoch 23/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.4663 - accuracy: 0.8306 - val_loss: 0.7140 - val_accuracy: 0.7015\n",
      "Epoch 24/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.4771 - accuracy: 0.8148 - val_loss: 0.5985 - val_accuracy: 0.7388\n",
      "Epoch 25/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.4325 - accuracy: 0.8359 - val_loss: 0.5463 - val_accuracy: 0.7873\n",
      "Epoch 26/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3895 - accuracy: 0.8609 - val_loss: 0.5296 - val_accuracy: 0.7910\n",
      "Epoch 27/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3764 - accuracy: 0.8701 - val_loss: 0.5623 - val_accuracy: 0.8022\n",
      "Epoch 28/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3712 - accuracy: 0.8649 - val_loss: 0.5538 - val_accuracy: 0.7910\n",
      "Epoch 29/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3734 - accuracy: 0.8517 - val_loss: 0.5107 - val_accuracy: 0.8172\n",
      "Epoch 30/30\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3426 - accuracy: 0.8860 - val_loss: 0.5181 - val_accuracy: 0.8097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8de4120370>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training a model with method '.compile()' and '.fit()'\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_norm, y_train, epochs=30, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8c3170",
   "metadata": {},
   "source": [
    "## NEED TO PLOT EPOCH, LOSS, and ACCURACY to easily see the overfitting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbad8f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - loss: 0.4595 - accuracy: 0.8063\n",
      "test_loss: 0.4594825506210327\n",
      "test_accuracy: 0.8063492178916931\n"
     ]
    }
   ],
   "source": [
    "# evaluate a model by using '.evaluate()' method\n",
    "test_loss, test_accuracy = model.evaluate(x_test_norm, y_test, verbose=2)\n",
    "print(f\"test_loss: {test_loss}\")\n",
    "print(f\"test_accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2be401",
   "metadata": {},
   "source": [
    "# 회고\n",
    "\n",
    "1회차\n",
    "- 학습 이미지로 300개가 있었고, 테스트 이미지도 300장이 있어서 노드에 있는 이미지들로 프로젝트를 진행했었다.\n",
    "- 결과가 30-40%대로 나왔어서 hyperparameter들을 만져보았다.\n",
    "    - epoch를 조금씩 올리자 테스트 결과가 조금씩 좋아졌지만, 어느 순간부터는 과적합에 빠져서 오히려 내려가는 결과를 보였다.\n",
    "    - 모델안에 hyperparameter들도 숫자를 올리면 조금씩 좋아지다가 내려가는 비슷한 경향을 보였다.\n",
    "    - 반대로 hyperparameter 숫자들을 내려봤는데, 올리는 것보다 조금 더 좋아지는 경향을 보였다.\n",
    "- 하지만 최대 40%후반에서 더 올라가지 않아서 정규화를 시도해보았다.\n",
    "    - 결과는 눈꼽만큼 좋아졌지, 엄청나게 높아지지는 않았다.\n",
    "- 계속 진행하다가, 우리 조 자체에서 다들 이미지를 취합해서 더 많은 데이터를 사용해보기로 결정을 했다.\n",
    "    - 그렇게 총 2100개의 이미지 데이터를 얻을 수 있었다.\n",
    "- 2100개의 데이터 자체를 sklearn.model_selection에 train_test_split을 이용하여 8:2로 나누었다.\n",
    "    - epoch를 20으로 늘리고 hyperparameter를 처음 모델 만들 때 그대로 진행했더니 쉽게 기준치를 넘는 점수를 보여주었다.\n",
    "    - 데이터가 많아짐에 따라 성능이 더 나온것으로 보인다\n",
    "\n",
    "2회차\n",
    "- 데이터 정규화, epoch=30, parameters 올리고 나서 최대 84%까지 증가\n",
    "    - Random state를 정하지 않아서 매번 다른 값이 나오기에 최대값만 기억함\n",
    "- sklearn에 train_test_split으로 train/validation/test로 나누어 보는 걸 해볼 예정\n",
    "    - 문제가 데이터를 나눈 후 사용하는 방법을 모르겠다...\n",
    "- 후에는 k-fold cross validation을 해볼 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8a4296",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
