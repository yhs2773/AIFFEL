{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b10c1e38",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f0f7c9",
   "metadata": {},
   "source": [
    "### Import file and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81917c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import glob  # glob function in glob module returns corresponding file names as list\n",
    "import tensorflow as tf\n",
    "import os, re \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b526d53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "# load file as raw data\n",
    "# '.getenv()' function returns string variable of all file in the path by adding '*' as arg\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path) # return all file names as list type\n",
    "\n",
    "raw_corpus = [] \n",
    "\n",
    "# assign all txt files and add it to raw_corpus\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines() # read() : bring data in a file to a single string.\n",
    "                                    # splitlines()  : split multi-line strings into single line list\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8778a0e9",
   "metadata": {},
   "source": [
    "### Regex (Regular Expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb2548d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "#     1. lowercase all letters and remove blank space\n",
    "#     2. add blank space on both sides of special characters\n",
    "#     3. change multiple blank spaces into single space\n",
    "#     4. If not, a-zA-Z?.!,¿ change to single space\n",
    "#     5. remove blank space\n",
    "#     6. add <start> at the start of the sentence and <end> at the end\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "# printing sample sentence\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885d4682",
   "metadata": {},
   "source": [
    "**Process of above cell (Regex) is similar to tensorflow \"Tokenizer or TextVectorization filtering\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "702e4a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>',\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding preprocessed sentences to a variable corpus\n",
    "corpus = []\n",
    "\n",
    "# Retrieve elements from raw_corpus list\n",
    "for sentence in raw_corpus:\n",
    "    # Skipping unwanted sentence\n",
    "    if len(sentence) == 0: continue   # skip if blank\n",
    "    if sentence[-1] == \":\": continue  # skip if sentence ends with :\n",
    "        \n",
    "    # use preprocess_sentence() function, transform sentence, and add them to corpus if less than 15\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    if len(preprocessed_sentence.split(\" \")) > 15: continue\n",
    "        \n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# quick check of first 10 elements from corpus\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d18dac0",
   "metadata": {},
   "source": [
    "**Process of above cell (picking out less than 15 words) is similar to tf.keras.preprocessing.sequence.pad_sequences(max_len)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f94300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-checking whether corpus contains lines that are less than 15 or not\n",
    "for i in corpus:\n",
    "    if len(i.split()) > 15:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5b360e",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4bbe02c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156013, 15)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Tokenizer and pad_sequences from tensorflow to break each element into tokens\n",
    "# Refer to below addresses for further info.\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, # set num of words to tokenize\n",
    "        filters=' ', # already filtered via preprocess_sentence function, hence ' ' to not use filter param\n",
    "        oov_token=\"<unk>\" # any words after 12000, turn them into '<unk>'\n",
    "    )\n",
    "    # creating tokenizer vocab from corpus\n",
    "    # tokenizer.fit_on_texts(texts): return string data into a list\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    # transform corpus into tensor using tokenizer\n",
    "    # tokenizer.texts_to_sequences(texts): return texts into num sequence\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    # equalizing the length of input data sequence\n",
    "    # add padding after each line if the length is not 15\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    # can use parameter maxlen to limit the length of a sentence\n",
    "    \n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b67361e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "# tokenizer.index_word: return dict of vocab (Ex. {index: '~~', index: '~~', ...})\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57d0476e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    4   95  303   62   53    9  946 6263    3    0    0    0]\n",
      "[  50    4   95  303   62   53    9  946 6263    3    0    0    0    0]\n",
      "14\n",
      "14\n",
      "156013\n",
      "(156013, 14)\n"
     ]
    }
   ],
   "source": [
    "# slice til last element of tensor as source input, which most likely will be <pad> not <end>\n",
    "src_input = tensor[:, :-1]  \n",
    "# target input will be anything after slicing <start> from tensor\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])\n",
    "print(len(src_input[0]))\n",
    "print(len(tgt_input[0]))\n",
    "print(len(src_input))\n",
    "print(src_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c9781d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "# tokenizer vocab = 12000 + 1 = 12001. Vocab size + 0:<pad>, which is equivalent to 12001\n",
    "# tokenizer.num_words: words based on 'n' frequency\n",
    "# tokenize() param of num_words = 12000 = tokenizer.num_words = 12000\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# Making dataset from prepped data source\n",
    "# Refer to below address for dataset (important)\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "232e6002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124810, 14) (31203, 14) (124810, 14) (31203, 14)\n"
     ]
    }
   ],
   "source": [
    "# splitting data into train and test sets\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2)\n",
    "print(enc_train.shape, enc_val.shape, dec_train.shape, dec_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19e6f43",
   "metadata": {},
   "source": [
    "# Building a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d28cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        # Consists of embedding layer, 2 LSTM layers, and 1 Dense layer\n",
    "        # Embedding layer changes vocab index value to corresponding index word vector\n",
    "        # this word vector is used as an abstract representation in definition of vector space\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) \n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)  \n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "# higher embedding size can pinpoint more abstract representation\n",
    "# but can lead to worse result if not enough data\n",
    "embedding_size = 256 # word vector dimension, size of abstract representation of the word\n",
    "hidden_size = 1024 # similar to how many workers you want to station\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
    "# tokenizer.num_words +1 is due to using <pad> which is not in the vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8195d599",
   "metadata": {},
   "source": [
    "### Training and testing a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64fe8c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "609/609 [==============================] - 119s 186ms/step - loss: 3.4190 - val_loss: 3.0525\n",
      "Epoch 2/10\n",
      "609/609 [==============================] - 113s 185ms/step - loss: 2.9630 - val_loss: 2.8378\n",
      "Epoch 3/10\n",
      "609/609 [==============================] - 113s 186ms/step - loss: 2.7926 - val_loss: 2.6910\n",
      "Epoch 4/10\n",
      "609/609 [==============================] - 113s 185ms/step - loss: 2.6677 - val_loss: 2.5730\n",
      "Epoch 5/10\n",
      "609/609 [==============================] - 113s 186ms/step - loss: 2.5604 - val_loss: 2.4685\n",
      "Epoch 6/10\n",
      "609/609 [==============================] - 113s 185ms/step - loss: 2.4635 - val_loss: 2.3719\n",
      "Epoch 7/10\n",
      "609/609 [==============================] - 113s 185ms/step - loss: 2.3730 - val_loss: 2.2822\n",
      "Epoch 8/10\n",
      "609/609 [==============================] - 113s 186ms/step - loss: 2.2887 - val_loss: 2.1994\n",
      "Epoch 9/10\n",
      "609/609 [==============================] - 113s 186ms/step - loss: 2.2096 - val_loss: 2.1231\n",
      "Epoch 10/10\n",
      "609/609 [==============================] - 113s 186ms/step - loss: 2.1358 - val_loss: 2.0467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4cd0ea3d60>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Refer to below addresses for optimizers and losses\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "\n",
    "# If unsure, which optimizer to use, choosing Adam can be a way\n",
    "# Need to train to reduce loss (loss/cost function)\n",
    "# Training to find a minimal value of loss function is the goal and process of finding it is optimization\n",
    "# optimizer is an algorithm of finding it\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# tf.keras.losses.SparseCategoricalCrossentropy\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy( \n",
    "    from_logits=True, reduction='none') \n",
    "# In classification, if using softmax, then param from_logits = False(default값), otherwise from_logits = True.\n",
    "\n",
    "# Steps to set up and train a model\n",
    "model.compile(loss=loss, optimizer=optimizer) # Setting loss function and optimizer\n",
    "model.fit(dataset, epochs=10, validation_data=(enc_val, dec_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11c11496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# summary of a model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75c947ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating generate_text function\n",
    "# Deliver <start> + alpha(if applicable) and the model will write/compose\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    \n",
    "    # for testing purposes, changing init_sentence as tensor\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # predict next word one by one to make a sentence\n",
    "    #    1. input received word's tensor\n",
    "    #    2. retrieve word index with the highest probability\n",
    "    #    3. concatenate predicted word index from step 2\n",
    "    #    4. if the model predicts <end> or reaches max_len, exit while loop\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4 \n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # use tokenizer to change word index back into human words\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated # return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e2da1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you liberian girl <end> '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyricist = model\n",
    "\n",
    "# with generate_text function, return a sentence after love\n",
    "generate_text(lyricist, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc6f06f",
   "metadata": {},
   "source": [
    "# 회고\n",
    "\n",
    "어려웠던 점은 프로젝트를 자세하게 이해하는 것이 힘들다는 것 이었다.\n",
    "\n",
    "텐서플로우를 이용해 모델 만드는 방법도 아직 서툴고 큰 흐름을 알지 못한채 이 프로젝트를 진행하려고 했다면 엄청나게 많은 시간이 필요했을 것 같다.\n",
    "\n",
    "다행히 코드를 작성하고 주석을 달면서 조금씩이나마 이해를 하기 시작했다는게 긍적적이었다.\n",
    "\n",
    "이번에 전처리 과정에서 regex를 이용해 문장을 미리 전처리 하고 문장 크기를 줄이는 과정이 있었다.\n",
    "\n",
    "예전에 regex를 배울땐 많이 사용해보지 못하여 이해하기가 어려웠는데, 다시 사용해보는 과정이 있어서 도움이 많이 됐다.\n",
    "\n",
    "더 쉽게 regex와 문장 수를 줄이는 방법을 찾다가 텐서플로우에 Tokenizer안에 filter와 pad_sequence안에 maxlen 파라미터를 이용하여 가능하다는 것을 새롭게 알게 됐다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ac47a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
