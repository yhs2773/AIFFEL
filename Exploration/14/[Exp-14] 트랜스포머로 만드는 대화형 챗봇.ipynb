{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30d4494c",
   "metadata": {},
   "source": [
    "# Load libraries and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a456a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os, re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0551167",
   "metadata": {},
   "source": [
    "## Load file and regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "778792b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "0                       12시 땡!                하루가 또 가네요.      0\n",
       "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
       "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
       "...                        ...                       ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load file\n",
    "filepath = os.getenv('HOME') + '/aiffel/transformer_chatbot/data/ChatbotData .csv'\n",
    "df = pd.read_csv(filepath)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e6ff2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex function\n",
    "def preprocess_sentence(sentence):\n",
    "    # whitespace between punctuations\n",
    "    # multiple whitespace into 1 whitespace\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "908c8810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex each column in dataframe\n",
    "df.Q = [preprocess_sentence(sentence) for sentence in df.Q]\n",
    "df.A = [preprocess_sentence(sentence) for sentence in df.A]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e4224ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 11823\n",
      "전체 샘플 수 : 11823\n"
     ]
    }
   ],
   "source": [
    "# unpack dataframe into 2 variables\n",
    "questions, answers = df.Q, df.A\n",
    "print('전체 샘플 수 :', len(questions))\n",
    "print('전체 샘플 수 :', len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "059e9e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후의 22번째 질문 샘플: 가스비 장난 아님\n",
      "전처리 후의 22번째 답변 샘플: 다음 달에는 더 절약해봐요 .\n"
     ]
    }
   ],
   "source": [
    "# print 22nd sapmle to cross-check\n",
    "print('전처리 후의 22번째 질문 샘플: {}'.format(questions[21]))\n",
    "print('전처리 후의 22번째 답변 샘플: {}'.format(answers[21]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa267eda",
   "metadata": {},
   "source": [
    "# Preprocess parallel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55e5f83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary using both questions and answers (must use both not just either one)\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7e570fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 'start token' and 'end token' in a temporary vocab with integer values\n",
    "# generate with last num + 1 and + 2 from total vocab size\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79cc2e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호 : [8364]\n",
      "END_TOKEN의 번호 : [8365]\n"
     ]
    }
   ],
   "source": [
    "# print int vals of start token and end token\n",
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5829e394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8366\n"
     ]
    }
   ],
   "source": [
    "# current vocab size is 8364 since start token index is 8364\n",
    "# since 2 new tokens added, vocab_size becomes + 2\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c34a8672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 21번째 질문 샘플: [5829, 605, 2500, 4174]\n",
      "정수 인코딩 후의 21번째 답변 샘플: [2685, 7669, 8, 6378, 95, 1]\n"
     ]
    }
   ],
   "source": [
    "# use tokenizer.encode() to change each word into integer vals\n",
    "# use tokenizer.decode() to change each integer vals into word sequence\n",
    "# example of 22nd sample\n",
    "\n",
    "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(questions[21])))\n",
    "print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(answers[21])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3653fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of Q: 57\n",
      "max length of A: 78\n"
     ]
    }
   ],
   "source": [
    "# find max length of questions and answers\n",
    "max_q = max([len(sentence) for sentence in questions])\n",
    "max_a = max([len(sentence) for sentence in answers])\n",
    "print('max length of Q:', max_q)\n",
    "print('max length of A:', max_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "989d41f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max length for pad_sequence (change this parameter to allow more sentences)\n",
    "MAX_LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78aa1dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encoding, skip any sentence over length 40, add padding\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "        # add start and end tokens during integer encoding\n",
    "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "        # add to dataset if sentence length is less than 41\n",
    "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence1)\n",
    "            tokenized_outputs.append(sentence2)\n",
    "  \n",
    "    # add padding to match length 40\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "    return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36feff1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 8366\n",
      "필터링 후의 질문 샘플 개수: 11823\n",
      "필터링 후의 답변 샘플 개수: 11823\n"
     ]
    }
   ],
   "source": [
    "# double check sample count and size\n",
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
    "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78639ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair Q&A and configure pipeline tf.data.Dataset API\n",
    "# for teach forcing, answers[:, :-1] as decoder input, answers[:, 1:] as decoder label\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# decoder uses last target as next input\n",
    "# remove start token from outputs\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c02aa56",
   "metadata": {},
   "source": [
    "# Positional encoding and attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11603117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional encoding layer\n",
    "# generate positional matrix to see visually\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        # create angle array\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "\n",
    "        # apply sin function on even index of array\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        # apply cosine function on odd index of array\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        # rearrange arrays to intersect sin and cosine\n",
    "        pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "        pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "        pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6422a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scaled dot product attention\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    # attention weight is the dot product of Q and K\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    # normalize weight\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # add mask to padding\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # apply softmax\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    # final attention is the dot product of the weight and V\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "497ceb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-head attention\n",
    "# call scaled dot product attention function internally\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "            'value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # apply dense to Q, K, V individually\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # make multi-head via parallel computation\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # scaled dot product attention function\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # concatenate results after attention computation\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "        # apply dense on the outputs\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3818ad6d",
   "metadata": {},
   "source": [
    "# Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f96236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num 0 from pad_seq is meaningless (just padding) so need to remove 'em during computation\n",
    "# padding masking checks position of num 0\n",
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "# input int seq, and this function returns num 0 in vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d0d93e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 0. 1. 0. 1.]]]\n",
      "\n",
      "\n",
      " [[[1. 1. 1. 0. 0.]]]], shape=(2, 1, 1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# quick cross check\n",
    "print(create_padding_mask(tf.constant([[1, 2, 0, 3, 0], [0, 0, 0, 4, 5]])))\n",
    "\n",
    "# prints 1 if val of idx is 0 else 0 if val of idx is actual num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8578193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look-ahead masking\n",
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ed1cbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1.]\n",
      "   [0. 0. 0. 0. 1.]\n",
      "   [0. 0. 0. 0. 0.]]]], shape=(1, 1, 5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# cross-check\n",
    "print(create_look_ahead_mask(tf.constant([[1, 2, 3, 4, 5]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "faefee98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[1. 1. 1. 1. 1.]\n",
      "   [1. 0. 1. 1. 1.]\n",
      "   [1. 0. 0. 1. 1.]\n",
      "   [1. 0. 0. 0. 1.]\n",
      "   [1. 0. 0. 0. 0.]]]], shape=(1, 1, 5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# need to pad any 0, hence calling create_padding_mask function internally\n",
    "print(create_look_ahead_mask(tf.constant([[0, 5, 1, 5, 5]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f58c73d",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ab04839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two sub-layers of 1 encoder layer\n",
    "# internally generate first sub-layer and second sub-layer\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "    # use padding mask\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # first sub-layer : multi-head attention (self-attention)\n",
    "    attention = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention\")({\n",
    "            'query': inputs,\n",
    "            'key': inputs,\n",
    "            'value': inputs,\n",
    "            'mask': padding_mask\n",
    "        })\n",
    "\n",
    "    # apply dropout and layer normalization to the attention result\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "    # second sub-layer : 2 fully-connected layer(FC)\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # apply dropout and layer normalization to the FC results\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e98390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect positional encoding and embedding layer and add num of encoding layers to successfully create transformer's encoder\n",
    "\n",
    "# transformer stacks num_layers (hyperparameter) of encoder layer\n",
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "    # padding mask\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # embedding layer\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # positional encoding\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    # num_layers amount of stacked encoder layers\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"encoder_layer_{}\".format(i),\n",
    "        )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9da1ac",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc3d2b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 sub-layers exist in 1 decoder layer\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # first sub-layer : multi-head attention (self-attention)\n",
    "    attention1 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "            'query': inputs,\n",
    "            'key': inputs,\n",
    "            'value': inputs,\n",
    "            'mask': look_ahead_mask\n",
    "        })\n",
    "\n",
    "    # apply layer normalization to the result of the multi-head attention\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    # second sub-layer : masked multi-head attention (encoder-decoder attention)\n",
    "    attention2 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "            'query': attention1,\n",
    "            'key': enc_outputs,\n",
    "            'value': enc_outputs,\n",
    "            'mask': padding_mask\n",
    "        })\n",
    "\n",
    "    # apply dropout and layer normalization to the masked multi-head attention results\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "    # third sub-layer : 2 FCs\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # apply dropout and layer normalization to the FC results\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91186e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect positional encoding and embedding layer and add num of decoding layers to successfully create transformer's encoder\n",
    "# transformer stacks num_layers (hyperparameter) of encoder layer\n",
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "    # padding mask\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # embedding layer\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # positinoal encoding\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    # apply dropout\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6013d035",
   "metadata": {},
   "source": [
    "# 모델 정의 및 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "071fc035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create transformer using encoder layer and decoder layer\n",
    "\n",
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    # padding mask for encoder\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='enc_padding_mask')(inputs)\n",
    "\n",
    "    # use look ahead mask for decoder's future tokens\n",
    "    # includes padding mask internally\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask,\n",
    "        output_shape=(1, None, None),\n",
    "        name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "    # mask encoder vectors on second attention block\n",
    "    # decoder mask for padding\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='dec_padding_mask')(inputs)\n",
    "\n",
    "    # encoder\n",
    "    enc_outputs = encoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "    # decoder\n",
    "    dec_outputs = decoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    # FC\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af92ac52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 512)    9017344     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 512)    12172288    dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8366)   4291758     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 25,481,390\n",
      "Trainable params: 25,481,390\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# hyperparameters\n",
    "NUM_LAYERS = 3 # num of layer of encoder and decoder\n",
    "D_MODEL = 512 # fixed I/O dim of encoder and decoder\n",
    "NUM_HEADS = 8 # num of heads in multi-head attention\n",
    "UNITS = 512 # hidden size of feed forward neural network\n",
    "DROPOUT = 0.2 # dropout ratio\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af14b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded in label sequence, apply padding mask during loss computation\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "369f2a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate is the most important hyperparameter during deep-learning training\n",
    "# exponentially increase learning rate at the beginning phase of model training\n",
    "# slowly decay learning rate as train step increases\n",
    "# this method is called Custom Learning rate Scheduling\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=10000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d061f617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualizing custom learning rate scheduling\n",
    "\n",
    "sample_learning_rate = CustomSchedule(d_model=128)\n",
    "\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01892c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model with loss function and custom learning rate\n",
    "\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03a80975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "185/185 [==============================] - 32s 125ms/step - loss: 1.4709 - accuracy: 0.0175\n",
      "Epoch 2/120\n",
      "185/185 [==============================] - 24s 127ms/step - loss: 1.2523 - accuracy: 0.0394\n",
      "Epoch 3/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 1.1061 - accuracy: 0.0495\n",
      "Epoch 4/120\n",
      "185/185 [==============================] - 23s 124ms/step - loss: 1.0315 - accuracy: 0.0500\n",
      "Epoch 5/120\n",
      "185/185 [==============================] - 23s 126ms/step - loss: 0.9913 - accuracy: 0.0507\n",
      "Epoch 6/120\n",
      "185/185 [==============================] - 23s 126ms/step - loss: 0.9617 - accuracy: 0.0522\n",
      "Epoch 7/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.9346 - accuracy: 0.0539\n",
      "Epoch 8/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.9078 - accuracy: 0.0555\n",
      "Epoch 9/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.8798 - accuracy: 0.0570\n",
      "Epoch 10/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.8500 - accuracy: 0.0589\n",
      "Epoch 11/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.8188 - accuracy: 0.0610\n",
      "Epoch 12/120\n",
      "185/185 [==============================] - 23s 126ms/step - loss: 0.7850 - accuracy: 0.0637\n",
      "Epoch 13/120\n",
      "185/185 [==============================] - 23s 126ms/step - loss: 0.7502 - accuracy: 0.0665\n",
      "Epoch 14/120\n",
      "185/185 [==============================] - 23s 126ms/step - loss: 0.7124 - accuracy: 0.0702\n",
      "Epoch 15/120\n",
      "185/185 [==============================] - 23s 126ms/step - loss: 0.6731 - accuracy: 0.0745\n",
      "Epoch 16/120\n",
      "185/185 [==============================] - 23s 126ms/step - loss: 0.6329 - accuracy: 0.0790\n",
      "Epoch 17/120\n",
      "185/185 [==============================] - 23s 126ms/step - loss: 0.5909 - accuracy: 0.0837\n",
      "Epoch 18/120\n",
      "185/185 [==============================] - 23s 126ms/step - loss: 0.5493 - accuracy: 0.0884\n",
      "Epoch 19/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.5054 - accuracy: 0.0943\n",
      "Epoch 20/120\n",
      "185/185 [==============================] - 23s 126ms/step - loss: 0.4616 - accuracy: 0.1001\n",
      "Epoch 21/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.4187 - accuracy: 0.1062\n",
      "Epoch 22/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.3768 - accuracy: 0.1122\n",
      "Epoch 23/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.3348 - accuracy: 0.1183\n",
      "Epoch 24/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.2945 - accuracy: 0.1242\n",
      "Epoch 25/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.2571 - accuracy: 0.1300\n",
      "Epoch 26/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.2214 - accuracy: 0.1356\n",
      "Epoch 27/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.1873 - accuracy: 0.1413\n",
      "Epoch 28/120\n",
      "185/185 [==============================] - 23s 126ms/step - loss: 0.1567 - accuracy: 0.1467\n",
      "Epoch 29/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.1297 - accuracy: 0.1516\n",
      "Epoch 30/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.1056 - accuracy: 0.1562\n",
      "Epoch 31/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0852 - accuracy: 0.1598\n",
      "Epoch 32/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0685 - accuracy: 0.1628\n",
      "Epoch 33/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0566 - accuracy: 0.1650\n",
      "Epoch 34/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0458 - accuracy: 0.1668\n",
      "Epoch 35/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0391 - accuracy: 0.1678\n",
      "Epoch 36/120\n",
      "185/185 [==============================] - 23s 126ms/step - loss: 0.0346 - accuracy: 0.1685\n",
      "Epoch 37/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0304 - accuracy: 0.1691\n",
      "Epoch 38/120\n",
      "185/185 [==============================] - 23s 126ms/step - loss: 0.0271 - accuracy: 0.1696\n",
      "Epoch 39/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0256 - accuracy: 0.1698\n",
      "Epoch 40/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0243 - accuracy: 0.1700\n",
      "Epoch 41/120\n",
      "185/185 [==============================] - 23s 126ms/step - loss: 0.0228 - accuracy: 0.1702\n",
      "Epoch 42/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0216 - accuracy: 0.1703\n",
      "Epoch 43/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0210 - accuracy: 0.1706\n",
      "Epoch 44/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0205 - accuracy: 0.1706\n",
      "Epoch 45/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0200 - accuracy: 0.1706\n",
      "Epoch 46/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0198 - accuracy: 0.1706\n",
      "Epoch 47/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0194 - accuracy: 0.1706\n",
      "Epoch 48/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0191 - accuracy: 0.1708\n",
      "Epoch 49/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0189 - accuracy: 0.1706\n",
      "Epoch 50/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0183 - accuracy: 0.1708\n",
      "Epoch 51/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0183 - accuracy: 0.1710\n",
      "Epoch 52/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0186 - accuracy: 0.1707\n",
      "Epoch 53/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0181 - accuracy: 0.1709\n",
      "Epoch 54/120\n",
      "185/185 [==============================] - 23s 126ms/step - loss: 0.0180 - accuracy: 0.1709\n",
      "Epoch 55/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0181 - accuracy: 0.1707\n",
      "Epoch 56/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0169 - accuracy: 0.1711\n",
      "Epoch 57/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0157 - accuracy: 0.1714\n",
      "Epoch 58/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0157 - accuracy: 0.1715\n",
      "Epoch 59/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0146 - accuracy: 0.1717\n",
      "Epoch 60/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0144 - accuracy: 0.1717\n",
      "Epoch 61/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0129 - accuracy: 0.1721\n",
      "Epoch 62/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0124 - accuracy: 0.1723\n",
      "Epoch 63/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0122 - accuracy: 0.1723\n",
      "Epoch 64/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0118 - accuracy: 0.1724\n",
      "Epoch 65/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0114 - accuracy: 0.1725\n",
      "Epoch 66/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0114 - accuracy: 0.1724\n",
      "Epoch 67/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0102 - accuracy: 0.1727\n",
      "Epoch 68/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0103 - accuracy: 0.1728\n",
      "Epoch 69/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0097 - accuracy: 0.1729\n",
      "Epoch 70/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0093 - accuracy: 0.1730\n",
      "Epoch 71/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0093 - accuracy: 0.1730\n",
      "Epoch 72/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0088 - accuracy: 0.1730\n",
      "Epoch 73/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0085 - accuracy: 0.1732\n",
      "Epoch 74/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0082 - accuracy: 0.1733\n",
      "Epoch 75/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0080 - accuracy: 0.1733\n",
      "Epoch 76/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0079 - accuracy: 0.1734\n",
      "Epoch 77/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0072 - accuracy: 0.1735\n",
      "Epoch 78/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0076 - accuracy: 0.1734\n",
      "Epoch 79/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0074 - accuracy: 0.1734\n",
      "Epoch 80/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0072 - accuracy: 0.1736\n",
      "Epoch 81/120\n",
      "185/185 [==============================] - 23s 126ms/step - loss: 0.0069 - accuracy: 0.1736\n",
      "Epoch 82/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0069 - accuracy: 0.1737\n",
      "Epoch 83/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0070 - accuracy: 0.1736\n",
      "Epoch 84/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0062 - accuracy: 0.1738\n",
      "Epoch 85/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0065 - accuracy: 0.1737\n",
      "Epoch 86/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0062 - accuracy: 0.1737\n",
      "Epoch 87/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0063 - accuracy: 0.1737\n",
      "Epoch 88/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0057 - accuracy: 0.1739\n",
      "Epoch 89/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0056 - accuracy: 0.1739\n",
      "Epoch 90/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0059 - accuracy: 0.1739\n",
      "Epoch 91/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0052 - accuracy: 0.1740\n",
      "Epoch 92/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0056 - accuracy: 0.1739\n",
      "Epoch 93/120\n",
      "185/185 [==============================] - 23s 126ms/step - loss: 0.0054 - accuracy: 0.1739\n",
      "Epoch 94/120\n",
      "185/185 [==============================] - 23s 126ms/step - loss: 0.0051 - accuracy: 0.1740\n",
      "Epoch 95/120\n",
      "185/185 [==============================] - 23s 126ms/step - loss: 0.0052 - accuracy: 0.1740\n",
      "Epoch 96/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0051 - accuracy: 0.1741\n",
      "Epoch 97/120\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0051 - accuracy: 0.1740\n",
      "Epoch 98/120\n",
      "161/185 [=========================>....] - ETA: 3s - loss: 0.0049 - accuracy: 0.1737"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2755/2726031429.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train model\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='accuracy', patience=5)\n",
    "EPOCHS = 120\n",
    "history = model.fit(dataset, epochs=EPOCHS, verbose=1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8cbce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model loss and accuracy\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['loss'],label='train')\n",
    "plt.title('Loss graph')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['accuracy'],label='train')\n",
    "plt.title('Accuracy graph')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c59ab4",
   "metadata": {},
   "source": [
    "# 챗봇 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64682bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference normally has below steps:\n",
    "# 1. same regex process with newly inputted sentence\n",
    "# 2. tokenize input sentence and add start token and end token\n",
    "# 3. compute padding masking and look ahead masking\n",
    "# 4. decoder predicts next word from input seq\n",
    "# 5. decoder adds predicted word to the former input seq and use it as next/new input\n",
    "# 6. when reached maximum set length of end token, decoder stops\n",
    "\n",
    "# create decoder_inference() function including above steps\n",
    "\n",
    "def decoder_inference(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    # change input sentence to int encoding and add start and end token\n",
    "    # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "    sentence = tf.expand_dims(\n",
    "        START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "    # continuously save predicted output sequence until current state\n",
    "    # nothing is predicted at the beginning, so only save start token ex) 8331\n",
    "    output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "    # decoder's inference step\n",
    "    for i in range(MAX_LENGTH):\n",
    "        # repeate word prediction until decoder reaches max_length\n",
    "        predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "\n",
    "        # current predicted word integer\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # when predicted word is end token, end for loop\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "            break\n",
    "\n",
    "        # constantly add predicted words to the output_sequence\n",
    "        # this output_sequence becomes decoder's input\n",
    "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output_sequence, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acf4cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence generation function to call decoder_inference from random input sentence\n",
    "\n",
    "def sentence_generation(sentence):\n",
    "    # return predicted int seq by actioning input sentence with decoder\n",
    "    prediction = decoder_inference(sentence)\n",
    "\n",
    "    # change int seq back to txt seq\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "        [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "    print('입력 : {}'.format(sentence))\n",
    "    print('출력 : {}'.format(predicted_sentence))\n",
    "\n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a09516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# receive answers from chatbots with inputting arbitrary sentence\n",
    "sentence_generation('안녕?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082418c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_generation(\"밥 먹었니?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90196a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_generation('내년 한국 경제 전망은 어떨 것 같니?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bb315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_generation('비트코인 사놨니?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d416ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_generation('사회주의에 대한 너의 생각은 알려줘.')"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "9de5437b",
   "metadata": {},
   "source": [
    "# 회고\n",
    "\n",
    "1회차\n",
    "- 노드에 파일을 불러오고, regex를 하고, 알아서 questions와 answers로 나눠주는 함수가 있었는데 인덱스 에러가 났다.\n",
    "    - 인덱스를 맞춰서 고쳤더니 아무 문장도 할당 되지 않았다.\n",
    "    - 데이터프레임으로 파일을 불러온 후, regex 진행 및 questions와 answer로 할당해 주었다.\n",
    "- 코드 구현을 다 진행한 후 첫번째 실험의 결과가 나왔다.\n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "- 실제 챗봇의 대답이 잘 나온것을 확인할 수 있었다.\n",
    "    - 간단한 질문은 잘 대답을 해주었다.\n",
    "    - 다만 학습을 진행할 때, 정확도가 16%까지만 올라가는 것을 볼수가 있었다.\n",
    "    - 다음 회차에서는 정확성을 높이는 방안을 생각해 보기로 하였다.\n",
    "    \n",
    "2회차\n",
    "- 정확성을 높이는 방법으로 학습률 및 hyper parameter를 변경하는 방안을 적용시키기로 했다.\n",
    "    - 먼저 층의 수를 2에서 3으로 조절해주었다.\n",
    "    - 좀 더 다양한 대답을 하였지만 크게 개선된 느낌은 아니었다.\n",
    "    - D_MODEL을 256에서 512로, DROPOUT을 0.1에서 0.2로 바꿔주고 진행해 보았다.\n",
    "- 정확도가 18%에 도착하지 않아서 다른 방법이 필요할 것 같다.\n",
    "    - warm_up steps를 4000에서 10000으로 바꿔봤다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ac1f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
