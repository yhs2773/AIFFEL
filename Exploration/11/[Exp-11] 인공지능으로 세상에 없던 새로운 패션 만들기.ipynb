{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c7214b2",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a13362bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import tensorflow as tf\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import PIL\n",
    "import time\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from matplotlib.pylab import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f519d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 3s 0us/step\n",
      "170508288/170498071 [==============================] - 3s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CIFAR-10 데이터셋도 tf.keras 안에 있는 datasets에 포함되어 있어서, 아래와 같이 손쉽게 데이터셋을 구성할 수 있습니다.\n",
    "# load data\n",
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(train_x, _), (test_x, _) = cifar10.load_data()\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d61aae2",
   "metadata": {},
   "source": [
    "# Configuring dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17073712",
   "metadata": {},
   "source": [
    "- 학습에 사용할 train_x의 이미지를 -1, 1로 정규화합니다.\n",
    "- 로드한 학습 데이터를 시각화를 통해 확인해 봅시다.\n",
    "- tf.data.Dataset 모듈의 from_tensor_slices() 함수를 사용하여 미니배치 데이터셋을 구성해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4237d15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1766d326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min 0\n",
      "max 255\n"
     ]
    }
   ],
   "source": [
    "print('min', train_x.min())\n",
    "print('max', train_x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bfbd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 사용할 train_x의 이미지를 -1, 1로 정규화합니다.\n",
    "train_x = (train_x - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a2dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(train_x[i].reshape(32, 32), cmap='gray')\n",
    "    plt.title(f'index: {i}')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ff3ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 12))\n",
    "\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    rd_index = np.random.randint(1, 60000)\n",
    "    plt.imshow(train_x[rd_index].reshape(32, 32), cmap='gray')\n",
    "    plt.title(f'index{rd_index}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cc8586",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_x).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa556d7",
   "metadata": {},
   "source": [
    "### STEP 3. 생성자 모델 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc15fe5",
   "metadata": {},
   "source": [
    "- (32, 32, 3)의 shape를 가진 이미지를 생성하는 생성자 모델 구현 함수를 작성해 봅시다.\n",
    "- noise = tf.random.normal([1, 100])로 생성된 랜덤 노이즈를 입력으로 하여 방금 구현한 생성자로 랜덤 이미지를 생성해 봅시다.\n",
    "- 생성된 랜덤 이미지가 생성자 출력 규격에 잘 맞는지 확인해 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c293242f",
   "metadata": {},
   "source": [
    "# Building generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b84f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator model function (model construction)\n",
    "def make_generator_model():\n",
    "\n",
    "    # Start\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # First: Dense layer\n",
    "    model.add(layers.Dense(8*8*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Second: Reshape layer\n",
    "    model.add(layers.Reshape((8, 8, 256)))\n",
    "\n",
    "    # Third: Conv2DTranspose layer\n",
    "    model.add(layers.Conv2DTranspose(128, kernel_size=(5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Fourth: Conv2DTranspose layer\n",
    "    model.add(layers.Conv2DTranspose(64, kernel_size=(5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Fifth: Conv2DTranspose layer\n",
    "    model.add(layers.Conv2DTranspose(3, kernel_size=(5, 5), strides=(2, 2), padding='same', use_bias=False, \\\n",
    "                                     activation='tanh'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a553a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of the generator model\n",
    "generator = make_generator_model() # assign model to generator\n",
    "generator.summary() # print summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47769de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating noise matrix to be used in generator\n",
    "noise = tf.random.normal([1, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b717901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assining generator to generated_image with noise data\n",
    "generated_image = generator(noise, training=False)\n",
    "generated_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d718411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting noise image (proto 1)\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32fd2df",
   "metadata": {},
   "source": [
    "### STEP 4. 판별자 모델 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c9b86b",
   "metadata": {},
   "source": [
    "- (32, 32, 3)의 이미지를 입력으로 받아 1dim을 판별결과를 출력하는 판별자 모델 구현 함수를 작성해 봅시다.\n",
    "- 위 STEP 2에서 생성한 랜덤 이미지를 판별자 모델이 판별한 결과값을 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0286e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator model function (model construction)\n",
    "def make_discriminator_model():\n",
    "\n",
    "    # Start\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # First: Conv2D Layer\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[32, 32, 3]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Second: Conv2D Layer\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Third: Flatten Layer\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Fourth: Dense Layer\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0984df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of the generator model\n",
    "discriminator = make_discriminator_model() # assign model to discriminator\n",
    "discriminator.summary() # print summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2363ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning discriminator to decision using generated_image\n",
    "decision = discriminator(generated_image, training=False)\n",
    "decision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34253d49",
   "metadata": {},
   "source": [
    "### STEP 5. 손실함수와 최적화 함수 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cd8dab",
   "metadata": {},
   "source": [
    "- 생성자와 판별자의 손실함수(loss)를 구현해 봅시다.\n",
    "- 판별자의 출력값을 가지고 실제/생성(real/fake) 이미지 판별 정확도(accuracy)를 계산하는 함수를 구현해 봅시다.\n",
    "- 생성자와 판별자를 최적화하는 optimizer를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e348193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function = binary cross entropy\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator loss function (cross entropy = 1 - fake_output)\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output) # wanting fake to be close to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f709a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator loss function\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output) # wanting real to be close to 1\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output) # wanting fake to be close to 0\n",
    "    total_loss = real_loss + fake_loss # adding real and fake\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5fc6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator accuracy function\n",
    "def discriminator_accuracy(real_output, fake_output):\n",
    "    real_accuracy = tf.reduce_mean(tf.cast(tf.math.greater_equal(real_output, tf.constant([0.5])), tf.float32))\n",
    "    fake_accuracy = tf.reduce_mean(tf.cast(tf.math.less(fake_output, tf.constant([0.5])), tf.float32))\n",
    "    return real_accuracy, fake_accuracy\n",
    "# ideally want both accuracy to be close to 0.5 instead of 1 or 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c69c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting optimizer\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f097dd47",
   "metadata": {},
   "source": [
    "### STEP 6. 훈련과정 상세 기능 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8138e4d3",
   "metadata": {},
   "source": [
    "- 1개 미니배치의 훈련 과정을 처리하는 train_step() 함수를 구현해 봅시다.\n",
    "- 16개의 고정된 seed를 입력으로 하여 훈련 과정 동안 생성한 이미지를 시각화하는 generate_and_save_images() 함수를 구현해 봅시다.\n",
    "- 훈련 epoch마다 생성자/판별자의 loss 및 판별자의 실제/생성(real/fake) 이미지 판별 accuracy 히스토리(history)를 그래프로 시각화하는 draw_train_history() 함수를 구현해 봅시다.\n",
    "- training_checkpoints 디렉토리에 몇 epoch마다 모델을 저장하는 checkpoint 모듈을 설정해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db45755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting fixed seed, if different cannot see/check the progress of the generated samples\n",
    "noise_dim = 100 # noise dimension\n",
    "num_examples_to_generate = 16 # num of samples to generate\n",
    "\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "seed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e33487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train step function with tensorflow decorator\n",
    "@tf.function\n",
    "def train_step(images):  # (1) Input data : Real Image 역할을 할 images 한 세트를 입력으로 받음\n",
    "    \n",
    "    # (2) 생성자 입력 노이즈 : generator가 FAKE IMAGE를 생성하기 위한 noise를 images 한 세트와 같은 크기인 BATCH_SIZE 만큼 생성함\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])  \n",
    "    \n",
    "    # (3) tf.GradientTape() : 가중치 갱신을 위한 Gradient를 자동 미분으로 계산하기 위해 오픈\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:  \n",
    "        # (4) generated_images 생성 : generator가 noise를 입력받은 후 generated_images 생성\n",
    "        generated_images = generator(noise, training=True)  \n",
    "\n",
    "        # (5) discriminator 판별 : discriminator가 Real Image인 images와 Fake Image인 generated_images를 \n",
    "        # 각각 입력받은 후 real_output, fake_output 출력\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        # (6) loss 계산 : fake_output, real_output으로 generator와 discriminator 각각의 loss 계산\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "        # (7) accuracy 계산 : fake_output, real_output으로 discriminator의 정확도 계산\n",
    "        real_accuracy, fake_accuracy = discriminator_accuracy(real_output, fake_output) \n",
    "    \n",
    "    # (8) gradient 계산 : gen_tape와 disc_tape를 활용해 gradient를 자동으로 계산\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    # (9) 모델 학습 : 계산된 gradient를 optimizer에 입력해 가중치 갱신\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss, real_accuracy, fake_accuracy  # (10) 리턴값 : 이번 스텝에 계산된 loss와 accuracy를 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae76676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, it, sample_seeds):\n",
    "\n",
    "    predictions = model(sample_seeds, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "   \n",
    "    plt.savefig('{}/aiffel/dcgan_newimage/fashion/generated_samples/sample_epoch_{:04d}_iter_{:03d}.png'\n",
    "                    .format(os.getenv('HOME'), epoch, it))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cd165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 15, 6    # matlab 차트의 기본 크기를 15,6으로 지정해 줍니다.\n",
    "\n",
    "def draw_train_history(history, epoch):\n",
    "    # summarize history for loss  \n",
    "    plt.subplot(211)  \n",
    "    plt.plot(history['gen_loss'])  \n",
    "    plt.plot(history['disc_loss'])  \n",
    "    plt.title('model loss')  \n",
    "    plt.ylabel('loss')  \n",
    "    plt.xlabel('batch iters')  \n",
    "    plt.legend(['gen_loss', 'disc_loss'], loc='upper left')  \n",
    "\n",
    "    # summarize history for accuracy  \n",
    "    plt.subplot(212)  \n",
    "    plt.plot(history['fake_accuracy'])  \n",
    "    plt.plot(history['real_accuracy'])  \n",
    "    plt.title('discriminator accuracy')  \n",
    "    plt.ylabel('accuracy')  \n",
    "    plt.xlabel('batch iters')  \n",
    "    plt.legend(['fake_accuracy', 'real_accuracy'], loc='upper left')  \n",
    "    \n",
    "    # training_history 디렉토리에 epoch별로 그래프를 이미지 파일로 저장합니다.\n",
    "    plt.savefig('{}/aiffel/dcgan_newimage/fashion/training_history/train_history_{:04d}.png'\n",
    "                    .format(os.getenv('HOME'), epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f34bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = os.getenv('HOME')+'/aiffel/dcgan_newimage/fashion/training_checkpoints'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3575677",
   "metadata": {},
   "source": [
    "### STEP 7. 학습 과정 진행하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27359583",
   "metadata": {},
   "source": [
    "- 위 STEP 5에서 구현한 기능들을 활용하여 최소 50 epoch만큼의 모델 학습을 진행해 봅시다.\n",
    "- 학습 과정에서 생성된 샘플 이미지로 만든 gif 파일을 통해 학습 진행 과정을 시각적으로 표현해 봅시다.\n",
    "- 학습 과정을 담은 샘플 이미지, gif 파일, 학습 진행 그래프 이미지를 함께 제출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff03681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs, save_every):\n",
    "    start = time.time()\n",
    "    history = {'gen_loss':[], 'disc_loss':[], 'real_accuracy':[], 'fake_accuracy':[]}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        for it, image_batch in enumerate(dataset):\n",
    "            gen_loss, disc_loss, real_accuracy, fake_accuracy = train_step(image_batch)\n",
    "            history['gen_loss'].append(gen_loss)\n",
    "            history['disc_loss'].append(disc_loss)\n",
    "            history['real_accuracy'].append(real_accuracy)\n",
    "            history['fake_accuracy'].append(fake_accuracy)\n",
    "\n",
    "            if it % 50 == 0:\n",
    "                display.clear_output(wait=True)\n",
    "                generate_and_save_images(generator, epoch+1, it+1, seed)\n",
    "                print('Epoch {} | iter {}'.format(epoch+1, it+1))\n",
    "                print('Time for epoch {} : {} sec'.format(epoch+1, int(time.time()-epoch_start)))\n",
    "\n",
    "        if (epoch + 1) % save_every == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator, epochs, it, seed)\n",
    "        print('Time for training : {} sec'.format(int(time.time()-start)))\n",
    "\n",
    "        draw_train_history(history, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65f5803",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_every = 5\n",
    "EPOCHS = 50\n",
    "\n",
    "# 사용가능한 GPU 디바이스 확인\n",
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b302723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train(train_dataset, EPOCHS, save_every)\n",
    "\n",
    "# 학습과정의 loss, accuracy 그래프 이미지 파일이 ~/aiffel/dcgan_newimage/fashion/training_history 경로에 생성되고 있으니\n",
    "# 진행 과정을 수시로 확인해 보시길 권합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc385996",
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_file = os.getenv('HOME')+'/aiffel/dcgan_newimage/fashion/fashion_mnist_dcgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "    filenames = glob.glob('{}/aiffel/dcgan_newimage/fashion/generated_samples/sample*.png'.format(os.getenv('HOME')))\n",
    "    filenames = sorted(filenames)\n",
    "    last = -1\n",
    "    for i, filename in enumerate(filenames):\n",
    "        frame = 2*(i**0.5)\n",
    "        if round(frame) > round(last):\n",
    "            last = frame\n",
    "        else:\n",
    "            continue\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "\n",
    "!ls -l ~/aiffel/dcgan_newimage/fashion/fashion_mnist_dcgan.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208e4132",
   "metadata": {},
   "source": [
    "(참고) 학습 과정 중 학습 epoch를 추가 진행해야 하거나, 학습한 모델을 활용하여 이미지를 생성할 필요가 생깁니다. 그럴 때마다 모델 학습을 처음부터 다시 진행한다면 시간 낭비가 될 것입니다.\n",
    "\n",
    "우리는 위에서 checkpoint 모듈을 이용해 모델을 저장해 둔 바 있습니다. 이를 이용해 학습해 둔 모델을 로드하면 모델 재학습이 필요 없이 이런 작업을 진행할 수 있습니다.\n",
    "\n",
    "아래는 checkpoint 모듈을 활용하여 모델을 로드하는 예시입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07d7b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = os.getenv('HOME')+'/aiffel/dcgan_newimage/cifar10/training_checkpoints'\n",
    "\n",
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "checkpoint.restore(latest)\n",
    "\n",
    "generator = checkpoint.generator\n",
    "discriminator = checkpoint.discriminator\n",
    "\n",
    "# 로드한 모델이 정상적으로 이미지를 생성하는지 확인해 봅니다. \n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "np_generated = generated_image.numpy()\n",
    "np_generated = (np_generated * 127.5) + 127.5   # reverse of normalization\n",
    "np_generated = np_generated.astype(int)\n",
    "plt.imshow(np_generated[0])\n",
    "plt.show()  # 정상적으로 모델이 로드되었다면 랜덤 이미지가 아니라 CIFAR-10 이미지가 그려질 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7903897f",
   "metadata": {},
   "source": [
    "checkpoint를 통해 모델이 로드되었으면 아래 코드를 실행할 경우 로드된 모델의 파라미터 상태에서 훈련을 재개할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbd9552",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train(train_dataset, EPOCHS, save_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1040137",
   "metadata": {},
   "source": [
    "### STEP 8. (optional) GAN 훈련 과정 개선하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d178d7b",
   "metadata": {},
   "source": [
    "- STEP 6을 진행하면서 생성된 샘플 이미지, 학습 과정 그래프 등을 통해 이전 훈련 과정의 문제점을 분석해 봅시다.\n",
    "- 모델구조 또는 학습 과정을 개선한 내역과 그 결과(샘플 이미지, 학습 과정 그래프 포함)를 함께 제출합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70d6528",
   "metadata": {},
   "source": [
    "(참고) 아래 언급된 페이지들에서 개선을 위한 아이디어를 얻을 수 있을 것입니다.\n",
    "\n",
    "- How to Train a GAN? Tips and tricks to make GANs work\n",
    "- 10 Lessons I Learned Training GANs for one Year\n",
    "- Tips for Training Stable Generative Adversarial Networks\n",
    "- Improved Techniques for Training GANs(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5422f9aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
